<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>leemay.dev</title>
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous">
    <link href="css/custom-ldev.css" rel="stylesheet">

  </head>

   <body>
    <main role="main" class="container" style="max-width:900px;">
      <h1 class="mt-5" style="color:#5993f3"> <a href="https://www.leemay.dev">leemay.dev</a></h1>

      <br/>
      <p class="lead" style="font-weight: 500">
      	The deep learning model that was trained on a 2013 dataset

		<br/>
  </p>
  <p>

<span style="font-weight: 1000; font-size:18px;color:#5993f3">tldr </span>
<br/>
A few years ago I worked on personalization for a video product. At our initial onset, we were a very small engineering team trying to navigate our way into the product so we could improve the content discovery experience for our users.

The first few months, we stumbled a bit. It was somewhat difficult figuring out what to work on with the few engineers we had that could give us the biggest bang for our buck. 

<br/><br/>
Because personalization's impact on the product was so far behind, what we worked on and the ROI of that work, was important. Not much time could be wasted on something that wasn't going to improve the user experience. 
<br/><br/>
So this is a story of a project we took on, a bit too early in our journey of rebuilding the team and the platform.  It took us a while to figure out what our home run would be (which I'll save for a later post); this unfortuntatly, was not it. 
  <hr>



<span style="font-size:18px;">Back in 2017,</span> we worked on a deep learning model for video recommendations (suggesting TV shows and movies based on your prior watch history) that was initially evaluated on an old static dataset from 2013‬. This deep learning model was originally developed by a peer research team on a Hadoop cluster that resided on physical infrastructure.  
<br/><br/>


‪Our first goal was to reproduce the evaluation metrics (i.e. recall) by training and evaluating (offline) the model in our AWS account, with intentions of not only comparing how long it took the model to train in our cloud environment but also to ensure we got the same recall numbers as the research team did in their stack. One key reason for initially reproducing the offline evaluation metrics with the old dataset in our stack, was to ensure that we had parody with the research team before we introduced any changes. 

<br/><br/>
<span style="font-size:18px;color:#5993f3"> Training the model on a fresh dataset </span>
<br/>
Once we accomplished training the model on the older dataset, we decided the next goal should be to re-evaluate the model on a fresh dataset. This fresh dataset was more representative of usage for our subscribers in the current timeframe. The model training logic accepted files that were a strange combination of comma, pipe and tab delimited records. We wrote a job that read in the current fresh dataset that consisted of Avro records and converted those Avro records to this strange format, instead of modifying the model training logic to read Avro‬. 

<br/><br/>
A few days later, we realized the format of the dataset for training had to be one large file...

<br/><br/>
With this realization, we then decided to rewrite the model training logic so that the input for training wasn’t that strange pipe, comma, tab, you-name-it format‬. 
<br/><br/>
Days were spent determining why the evaluation numbers were lower for the model trained on the old dataset vs the fresh / current dataset. 
<br/><br/>

<span style="font-size:18px;color:#5993f3"> And so..we gave up </span>
<br/>
We eventually pivoted away from this deep learning model and opted for focusing efforts on introducing a 'For You' personalized homepage using an older collaborative filtering algorithm that was already available in our production environment. 
<br/><br/>

During those months, maybe time would have been better spent working on the deficiencies in the data pipeline that resulted in opting for the old poorly formatted 2013 dataset initially (so that our research peers always had a fresh dataset to work with). Or maybe time would have been better spent going straight to production with the collaborative filtering algorithm and then introduce a more complex model like deep learning into production; using the collobrative filtering algorithm as the baseline. 

<br/><br/>
Back in 2017, it felt like months were wasted on trying to get this deep learning model trained on a fresh dataset that was more representative of production. 

<br/><br/>
Reflecting on this now, it still feels like months were wasted.<span style="font-weight: 900"> Our engineering team was small, so focusing on the right work was important. </span> Sometimes starting simple, executing, then iterating and improving is the best appoarch. 

<br/><br/>
Note: I no longer work on video recommendations, so for all I know, this deep learning model could be serving millions of requests today. 

      </p>
 
   	  <br/>

      
    </main>

    <footer class="footer">
      <div class="container">
        <span>
        	<a href="https://www.linkedin.com/in/leemaynassery/" target="_blank" class="foot-text">LinkedIn </span>
			<a href="https://twitter.com/leemaynassery" target="_blank" class="foot-text"> Twitter</span>
        </span>
      </div>
    </footer>
  </body>

</html>


    




